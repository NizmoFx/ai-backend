# AI Aided Asset Versioning Platform — Backend (Part 1)

FastAPI + PostgreSQL + Celery (RabbitMQ) backend implementing:
- Authentication (token demo)
- Projects & Assets upload/management
- Generation jobs (async) with a **provider-agnostic AI layer** (OpenAI/Gemini/Dummy)
- Local storage for assets (S3 in a later phase)
- OpenAPI/Swagger auto-generated by FastAPI
- Dockerfile + docker-compose for local testing
- Functional test covering the user flow

> **Note:** OpenAI/Gemini providers are wired but optional. By default the `DummyProvider`
performs operations using Pillow (resize/crop/text overlay) to keep the pipeline testable
without external keys. Switch providers via `AI_PROVIDER` env var.

## Quickstart (Docker Compose)

```bash
# 1) Copy env template and adjust as needed
cp .env.example .env

# 2) Build & run
docker compose up --build

# Services:
# - api: FastAPI on http://localhost:8000/docs
# - db: Postgres
# - mq: RabbitMQ management UI on http://localhost:15672 (guest/guest)
# - worker: Celery worker

# 3) Run test (optional, in API container):
docker compose exec api pytest -q
```

## Environment Variables (.env)
- `DATABASE_URL` default: `postgresql+psycopg2://postgres:postgres@db:5432/app`
- `AI_PROVIDER` one of: `dummy` (default), `openai`, `gemini`
- `OPENAI_API_KEY` (if using openai)
- `GEMINI_API_KEY` (if using gemini)
- `SECRET_KEY` a random string for token signing
- `STORAGE_DIR` default: `/app/storage`
- `RABBITMQ_URL` default: `amqp://guest:guest@mq:5672//`

## Minimal User Flow
1. `POST /auth/login` → returns demo token
2. `POST /projects` → create project
3. `POST /projects/{id}/assets` (multipart) → upload an image asset
4. `POST /generation/start` with asset_id + operations → returns job_id
5. `GET /generation/status/{job_id}` → polls status; on `done`, get result path
6. download the generated file via `GET /assets/{asset_id}` (or result path)

## Notes for Reviewers
- Provider abstraction in `app/services/ai`: `base.py`, `dummy_provider.py`, `openai_provider.py`, `gemini_provider.py`.
- Celery task `process_generation` orchestrates provider operations and writes output to STORAGE_DIR.
- Schema & migrations are simple SQLAlchemy models; this is a part‑1 implementation meant to validate the pipeline.

## Extras added
- **Admin Presets**: `/presets` CRUD + `/generation/start_by_preset?asset_id=&preset_id=`
- **Basic Moderation**: file type & size checks on upload
- **Structured logging**: consistent INFO logs
- **Seed script**: `python scripts/seed_presets.py`

